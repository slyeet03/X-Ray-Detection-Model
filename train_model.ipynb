{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377d99df",
   "metadata": {},
   "source": [
    "# Setup and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9032181",
   "metadata": {},
   "source": [
    "## Install dependencies and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install tensorflow-macos tensorflow-metal opencv-python matplotlib panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a47982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82553fcf",
   "metadata": {},
   "source": [
    "### Check whether gpu available or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691f3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    print(\"✅ Using GPU:\", gpus)\n",
    "else:\n",
    "    print(\"❌ No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffe5fe",
   "metadata": {},
   "source": [
    "## Load Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf1a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "train_df = pd.read_csv(\"CheXpert-v1.0-small/train.csv\")\n",
    "valid_df = pd.read_csv(\"CheXpert-v1.0-small/valid.csv\")\n",
    "\n",
    "LABELS = [\"Pneumothorax\", \"Pneumonia\", \"Edema\", \"Pleural Effusion\", \"Consolidation\", \"Cardiomegaly\", \"Atelectasis\"]\n",
    "\n",
    "# making dataset binary\n",
    "train_df[LABELS] = train_df[LABELS].fillna(0).replace(-1, 0)\n",
    "valid_df[LABELS] = valid_df[LABELS].fillna(0).replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c519ad",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29463b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(image, label):\n",
    "    # Random brightness adjustment\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    \n",
    "    # Random contrast adjustment\n",
    "    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "    \n",
    "    # Random flip (horizontal only)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    # Random rotation\n",
    "    angle = tf.random.uniform([], minval=-0.1, maxval=0.1)  # ±~5.7 degrees\n",
    "    image = tf.keras.preprocessing.image.apply_affine_transform(\n",
    "        image.numpy(), theta=angle * 180 / np.pi, fill_mode='constant'\n",
    "    )\n",
    "    image = tf.convert_to_tensor(image)\n",
    "    \n",
    "    # Ensure values stay in [0,1] range\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625de41b",
   "metadata": {},
   "source": [
    "## Dataset for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def parse_image(filename, label):\n",
    "    img=tf.io.read_file(filename)\n",
    "    # decoding jpg to tensor in 3 channels\n",
    "    img=tf.image.decode_jpeg(img,channels=3)\n",
    "    img=tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
    "    # normalizing [0,255] to [0,1]\n",
    "    img=tf.cast(img, tf.float32)/255.0\n",
    "    return img, label\n",
    "\n",
    "def df_to_dataset(df, base_dir=\"CheXpert-v1.0-small\", training=False):\n",
    "    # adding filepath to the img\n",
    "    filepaths = df[\"Path\"].apply(lambda x: os.path.join(base_dir, x))\n",
    "    # only takes images with the labels defined above\n",
    "    labels = df[LABELS].values.astype(\"float32\")\n",
    "    # makes a dataset that pairs each label with their image\n",
    "    ds=tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
    "    # apply parse image in parallel processing to make it faster\n",
    "    ds=ds.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # shuffle the images and make it into batches\n",
    "    ds=ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds=df_to_dataset(train_df, training=True)\n",
    "valid_ds=df_to_dataset(valid_df, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c15d55",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready-made brain --> resnet50 already knows detection of edges and shapes and textures so we resue that knowledge with our dataset\n",
    "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# freeze at start so it doesnt forget basic knowledsge\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    # squashing all the detected features into vectors\n",
    "    base_model, layers.GlobalAveragePooling2D(),\n",
    "    # every iteration it turns some neurons off so that every neuron learns instead of relying on sm neurons\n",
    "    layers.Dropout(0.5),\n",
    "    # for every disease giving 0 and 1\n",
    "    # using sigmoid instead of softmax for multilabeling\n",
    "    layers.Dense(len(LABELS), activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Calculate class weights\n",
    "# increases the cost of getting something wrong wrt how rare they are so the model doesnt get a easy way out\n",
    "class_totals = train_df[LABELS].sum().values\n",
    "total_samples = len(train_df)\n",
    "class_weights = total_samples / (len(LABELS) * class_totals)\n",
    "class_weight_dict = {i: class_weights[i] for i in range(len(LABELS))}\n",
    "\n",
    "\n",
    "# adam tell model how to update itself, loss --> tells the model how wrong it is, metrics --> keeps track of how well the model is doing\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45af4b",
   "metadata": {},
   "source": [
    "# Adding callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"best_model.keras\",\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50811a9",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6292f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit --> training begins\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    class_weight=class_weight_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c64ff6f",
   "metadata": {},
   "source": [
    "## Unfreeze some layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=True\n",
    "\n",
    "# finetuning\n",
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506caa8",
   "metadata": {},
   "source": [
    "## Recompiling with smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf352be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c29ab3f",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=valid_ds,\n",
    "    epochs=50,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599adaad",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7a5aa",
   "metadata": {},
   "source": [
    "# Plotting the model stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c2437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(histories, titles):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1,2,1)\n",
    "    for h, t in zip(histories, titles):\n",
    "        plt.plot(h.history['loss'], label=f'{t} train loss')\n",
    "        plt.plot(h.history['val_loss'], label=f'{t} val loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    for h, t in zip(histories, titles):\n",
    "        plt.plot(h.history['accuracy'], label=f'{t} train acc')\n",
    "        plt.plot(h.history['val_accuracy'], label=f'{t} val acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_history([history, history_fine], ['Initial', 'Fine-tuning'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
